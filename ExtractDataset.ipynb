{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Abstracts of Academic Articles\n",
    "\n",
    "Topics/Keywords: climate change, environment, sustainability, pollution, global warming, sea level rise, climate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Config as CON\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import ujson as json\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('tagsets')\n",
    "\n",
    "#Declare some necessary global functions and objects\n",
    "current_time_ms = lambda: int(round(time.time() * 1000))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize any Keyword\n",
    "\n",
    "When we try to match keywords, we try to match them in their normalized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_keyword(keyword):\n",
    "    \"\"\"\n",
    "    Given a keyword, convert it to a normalized form so that it can be compared with others in a more robust way\n",
    "    keyword: string\n",
    "    output: string\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower()\n",
    "    #Remove punctuations\n",
    "    text=keyword.translate((str.maketrans('','',string.punctuation)))\n",
    "    #Tokenize\n",
    "    text_tokens = word_tokenize(text)\n",
    "    #Remove stopwords\n",
    "    text_tokens = [word for word in text_tokens if not word in stop_words]\n",
    "    #Lemmatize tokens\n",
    "    text_tokens = [lemmatizer.lemmatize(text_token) for text_token in text_tokens]\n",
    "    \n",
    "    norm_keyword = \"\"\n",
    "    for token in text_tokens:\n",
    "        norm_keyword +=token\n",
    "    return norm_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations to be removed: \n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "----------------------------\n",
      "\n",
      "machinelearning\n",
      "humancomputerinteraction\n",
      "mig23\n",
      "learninginteractionapplication\n",
      "[('I', 'PRP'), ('eat', 'VBP'), ('rice', 'NN'), ('while', 'IN'), ('playing', 'VBG'), ('video', 'NNS'), ('games', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "#How to apply normalization\n",
    "\n",
    "print(\"Punctuations to be removed: \")\n",
    "print(string.punctuation)\n",
    "print(\"----------------------------\\n\")\n",
    "\n",
    "keyword = \"Machine Learning\"\n",
    "print(normalized_keyword(keyword))\n",
    "keyword = \"Human-Computer-Interaction\"\n",
    "print(normalized_keyword(keyword))\n",
    "keyword = \"Mig#23\"\n",
    "print(normalized_keyword(keyword))\n",
    "keyword = \"learning, interaction, and application\"\n",
    "print(normalized_keyword(keyword))\n",
    "\n",
    "#POS-tagging\n",
    "print(nltk.pos_tag(['I','eat','rice','while','playing','video','games']))\n",
    "\n",
    "#Details about the POS tags can be found here\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Search Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climatechange', 'sustainability', 'pollution', 'globalwarming', 'sealevelrise', 'climate', 'waterstress', 'coastalflooding']\n"
     ]
    }
   ],
   "source": [
    "search_keys = [\"climate change\", \"sustainability\", \"pollution\", \"global warming\", \"sea level rise\", \"climate\", \"water stress\", \"coastal flooding\"]\n",
    "norm_search_keys = [normalized_keyword(key) for key in search_keys]\n",
    "print(norm_search_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filereader for Multiprocessing Environement\n",
    "\n",
    "We want to process each input files independently first. Our goal is to produce an independent list of abstracts that match search keywords, considering only the local file.\n",
    "\n",
    "<b>Input</b>: filename (absolute path), pid (process id for managing temporary files that will be used to merge the outputs)\n",
    "\n",
    "<b>Output</b>: Tuples (keywords, abstract) in a json file <u>\"tempAbstracts_[pid].json\"</u>; total_papers: number of papers in this file; papers_with_keywords: number of papers that have matching keywords\n",
    "\n",
    "<b>JSON object</b>: {keywords: [\"a\",\"b\"], abstract: \"this will contain the paper abstract\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readOAGPaper(file_absolute_path, pid):\n",
    "    start = current_time_ms()\n",
    "    filename = file_absolute_path\n",
    "    outfilename = CON.TEMP_DATA_DIRECTORY + \"tempAbstracts_\"+str(pid)+\".json\"\n",
    "    \n",
    "    abstracts = []\n",
    "    #norm_keyword_to_original = what is the human readable form of the normalized_keyword?\n",
    "    norm_keyword_to_original = {}\n",
    "    \n",
    "    total_papers = 0\n",
    "    papers_with_keywords = 0\n",
    "    \n",
    "    f = open(filename,mode=\"r\",encoding=\"utf-8\")\n",
    "    lineNo = 0\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        lineNo +=1\n",
    "        \n",
    "        #Comment these two lines when running in server\n",
    "        #if lineNo == 5001:\n",
    "        #    break\n",
    "            \n",
    "        paper = json.loads(line)\n",
    "        attrs = paper.keys()\n",
    "            \n",
    "        #Discard papers of foreign language\n",
    "            \n",
    "        #Language is already detected in the dataset -- accuracy is low.\n",
    "        #if 'lang' in attrs:\n",
    "        #    if paper['lang']!='en':\n",
    "        #        print(paper['abstract'])\n",
    "        #        continue\n",
    "            \n",
    "        #Try to detect language from abstract\n",
    "        if 'abstract' in attrs:\n",
    "            try:\n",
    "                lang = detect(paper['abstract'])\n",
    "                    \n",
    "                if lang !='en':\n",
    "                    #Skip the paper if the language of abstract is not english\n",
    "                    continue\n",
    "            except:\n",
    "                #Skip the paper if the language of abstract cannote be detected\n",
    "                continue\n",
    "        else:\n",
    "            #Skip the paper if the abstract is not available\n",
    "            continue\n",
    "        \n",
    "        if len(paper['abstract'])<100 or len(paper['abstract'])>5000:\n",
    "            continue\n",
    "        \n",
    "        ##Language test passed\n",
    "        total_papers +=1\n",
    "        matched = False\n",
    "        if 'keywords' in attrs:\n",
    "            for keyword in paper['keywords']:\n",
    "                if matched:\n",
    "                    break\n",
    "                    \n",
    "                norm_keyword = normalized_keyword(keyword)\n",
    "                \n",
    "                for search_key in norm_search_keys:\n",
    "                    if search_key in norm_keyword:\n",
    "                        json_object = {}\n",
    "                        json_object[\"keywords\"] = paper['keywords']\n",
    "                        json_object[\"abstract\"] = paper['abstract']\n",
    "                        papers_with_keywords +=1\n",
    "                        abstracts.append(json_object)\n",
    "                        matched = True\n",
    "                        break\n",
    "                    #\n",
    "                #\n",
    "            #\n",
    "        #\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    #Print the list of abstracts in temporary json files\n",
    "    #TO-DO\n",
    "    #print(abstracts)\n",
    "    with open(outfilename, 'w') as fout:\n",
    "        json.dump(abstracts, fout)\n",
    "    \n",
    "    end = current_time_ms()\n",
    "    \n",
    "    print(\"Processing time [Process %d]: %d ms\"%(pid, end-start))\n",
    "    \n",
    "    return (total_papers, papers_with_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing Control Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading OAG Paper Files\n",
      "-------------------------\n",
      "Number of files: 15\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_0.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_1.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_2.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_3.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_4.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_5.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_6.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_7.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_8.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_9.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_10.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_11.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_12.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_13.json\n",
      "Deleting /fred/oz130/comm_search/trend-analysis/Temp/tempAbstracts_14.json\n",
      "Processing time [Process 7]: 1050 ms\n",
      "Processing time [Process 3]: 1385 ms\n",
      "Processing time [Process 4]: 1597 ms\n",
      "Processing time [Process 2]: 1600 ms\n",
      "Processing time [Process 1]: 18362 ms\n",
      "Processing time [Process 0]: 18447 ms\n",
      "Processing time [Process 5]: 18616 ms\n",
      "Processing time [Process 6]: 19377 ms\n",
      "Processing time [Process 11]: 15283 ms\n",
      "Processing time [Process 14]: 18463 ms\n",
      "Processing time [Process 10]: 19277 ms\n",
      "Processing time [Process 9]: 29452 ms\n",
      "Processing time [Process 8]: 32945 ms\n",
      "Processing time [Process 13]: 43291 ms\n",
      "Processing time [Process 12]: 44677 ms\n",
      "All sub-processes terminated\n",
      "Total papers: 31167\n",
      "Papers with keywords: 234\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    global_start = current_time_ms()\n",
    "    \n",
    "    if not os.path.exists(CON.TEMP_DATA_DIRECTORY):\n",
    "        os.mkdir(CON.TEMP_DATA_DIRECTORY)\n",
    "    \n",
    "    #Define number of parallel processes\n",
    "    num_workers = CON.NUM_POOLS\n",
    "    #Each new worker will be assigned a pid (auto-incremented)\n",
    "    pid = 0\n",
    "    \n",
    "    #List OAG Paper Files\n",
    "    print(\"Reading OAG Paper Files\")\n",
    "    print(\"-------------------------\")\n",
    "    CD = CON.INPUT_DATA_DIRECTORY + \"OAG_Papers\"\n",
    "    oag_paper_files = [ (CD+\"/\"+f) for f in os.listdir(CD) if os.path.isfile(CD+\"/\"+f)]\n",
    "    print(\"Number of files: %d\"%(len(oag_paper_files)))\n",
    "    \n",
    "    temp_abstract_files = [CON.TEMP_DATA_DIRECTORY+\"tempAbstracts_\"+str(i)+\".json\" for i in range(0,len(oag_paper_files))]\n",
    "    \n",
    "    temp_files = temp_abstract_files\n",
    "    #Clear temporary files created by previous instance of this code\n",
    "    for filename in temp_files:\n",
    "        if os.path.exists(filename):\n",
    "            print(\"Deleting \"+filename)\n",
    "            os.remove(filename)\n",
    "    \n",
    "    total_papers = 0\n",
    "    papers_with_keywords = 0\n",
    "    pool = mp.Pool()\n",
    "    num_rounds = math.ceil(len(oag_paper_files)/num_workers)\n",
    "    last_round_files = len(oag_paper_files) - (num_rounds-1)*num_workers\n",
    "    \n",
    "    for round_no in range(0,num_rounds-1):\n",
    "        pool_results = []\n",
    "        for w in range(0, num_workers):\n",
    "            index = round_no*num_workers + w\n",
    "            pool_results.append(pool.apply_async(readOAGPaper,args=(oag_paper_files[index], index,)))\n",
    "    \n",
    "        #Wait for the termination\n",
    "        for pool_result in pool_results:\n",
    "            (tp, pwk) = pool_result.get()\n",
    "            total_papers +=tp\n",
    "            papers_with_keywords +=pwk\n",
    "        \n",
    "        pool_results.clear()\n",
    "        \n",
    "    pool_results = []\n",
    "    for w in range(0, last_round_files):\n",
    "        index = (num_rounds-1)*num_workers + w\n",
    "        pool_results.append(pool.apply_async(readOAGPaper,args=(oag_paper_files[index],index,)))\n",
    "        \n",
    "    #Wait for the termination\n",
    "    for pool_result in pool_results:\n",
    "        (tp, pwk) = pool_result.get()\n",
    "        total_papers +=tp\n",
    "        papers_with_keywords +=pwk\n",
    "        \n",
    "    pool_results.clear()\n",
    "    \n",
    "    print(\"All sub-processes terminated\")\n",
    "    print(\"Total papers: %d\"%(total_papers))\n",
    "    print(\"Papers with keywords: %d\"%(papers_with_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Back and Combine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset collected successfully\n",
      "Number of articles 234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert(os.path.exists(CON.TEMP_DATA_DIRECTORY))\n",
    "all_abstracts = []\n",
    "count = 0\n",
    "for filename in os.listdir(CON.TEMP_DATA_DIRECTORY):\n",
    "    with open(os.path.join(CON.TEMP_DATA_DIRECTORY, filename), encoding='utf-8') as data_file:\n",
    "        data = json.loads(data_file.read())\n",
    "        all_abstracts.extend(data)\n",
    "        count += len(data)\n",
    "    \n",
    "if not os.path.exists(CON.OUTPUT_DATA_DIRECTORY):\n",
    "    os.mkdir(CON.OUTPUT_DATA_DIRECTORY)\n",
    "\n",
    "output_filename = os.path.join(CON.OUTPUT_DATA_DIRECTORY,\"all_abstracts_with_keywords.json\")\n",
    "\n",
    "with open(output_filename, 'w') as fout:\n",
    "    json.dump(all_abstracts, fout)\n",
    "    \n",
    "print(\"Dataset collected successfully\")\n",
    "print(\"Number of articles %d\\n\"%len(all_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
